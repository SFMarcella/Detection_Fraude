{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "23761e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import regularizers\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Réglages\n",
    "\n",
    "DATA_PATH = \"new_data.csv\"\n",
    "CHUNKSIZE = 500_000\n",
    "RESULTS_CSV = \"douane_test_results.csv\"\n",
    "\n",
    "def safe_div(a, b):\n",
    "    return a / (b + 1e-9)\n",
    "\n",
    "# Supprimer ancien fichier résultats\n",
    "if os.path.exists(RESULTS_CSV):\n",
    "    os.remove(RESULTS_CSV)\n",
    "\n",
    "first_chunk = True\n",
    "all_results = []\n",
    "\n",
    "for chunk in pd.read_csv(DATA_PATH, delimiter=\";\", encoding=\"latin1\", chunksize=CHUNKSIZE):\n",
    "    df = chunk.copy()\n",
    "\n",
    "    # Colonnes pertinentes\n",
    "    cols_valeur = [\n",
    "        \"assessed_value_usd\", \"value_per_kg\", \"net_weight_kg\", \"gross_weight_kg\",\n",
    "        \"AVG_FOB_x\", \"AVG_FOB_y\", \"AVG_TAX_x\", \"AVG_TAX_y\",\n",
    "        \"AVG_FOB_3MTH\", \"AVG_TAX_3MTH\", \"DECL_FREQ_x\", \"DECL_FREQ_y\"\n",
    "    ]\n",
    "    for c in cols_valeur:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0.0\n",
    "\n",
    "    df_valeur = df[cols_valeur].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4d3820bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Feature engineering\n",
    "df_valeur[\"val_per_net_weight\"] = df[\"assessed_value_usd\"] / (df[\"net_weight_kg\"] + 1e-6) #prix unitaire par kg\n",
    "df_valeur[\"val_vs_avg_fob_x\"]   = df[\"assessed_value_usd\"] / (df[\"AVG_FOB_x\"] + 1e-6) #comparaison avec prix moyens de référence.\n",
    "df_valeur[\"val_vs_avg_fob_y\"]   = df[\"assessed_value_usd\"] / (df[\"AVG_FOB_y\"] + 1e-6)\n",
    "df_valeur[\"gross_net_ratio\"]    = (df[\"gross_weight_kg\"] + 1e-6) / (df[\"net_weight_kg\"] + 1e-6) #cohérence poids brut / net.\"\"\"\n",
    "\n",
    "# Feature engineering\n",
    " \n",
    "df_valeur[\"val_per_net_weight\"] = safe_div(df[\"assessed_value_usd\"], df[\"net_weight_kg\"])\n",
    "df_valeur[\"val_vs_avg_fob_x\"] = safe_div(df[\"assessed_value_usd\"], df[\"AVG_FOB_x\"])\n",
    "df_valeur[\"val_vs_avg_fob_y\"] = safe_div(df[\"assessed_value_usd\"], df[\"AVG_FOB_y\"])\n",
    "df_valeur[\"gross_net_ratio\"] = safe_div(df[\"gross_weight_kg\"], df[\"net_weight_kg\"])\n",
    "\n",
    "# Nettoyage\n",
    "\n",
    "df_valeur.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_valeur.fillna(0, inplace=True)\n",
    "\n",
    "# Standardisation\n",
    "\n",
    "if first_chunk:\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df_valeur)\n",
    "    joblib.dump(scaler, \"scaler.pkl\")\n",
    "else:\n",
    "    scaler = joblib.load(\"scaler.pkl\")\n",
    "    X_scaled = scaler.transform(df_valeur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4476f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest\n",
    "\n",
    "if first_chunk:\n",
    "    iso = IsolationForest(n_estimators=200, contamination=0.05, random_state=42)\n",
    "    iso.fit(X_scaled)\n",
    "    joblib.dump(iso, \"isolation_forest_model.pkl\")\n",
    "else:\n",
    "    iso = joblib.load(\"isolation_forest_model.pkl\")\n",
    "df[\"iso_pred\"] = iso.predict(X_scaled)\n",
    "df[\"iso_pred\"] = df[\"iso_pred\"].map({-1: 1, 1: 0})\n",
    "df[\"iso_score\"] = iso.decision_function(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4f43b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Class SVM\n",
    "\n",
    "if first_chunk:\n",
    "    ocsvm = OneClassSVM(kernel=\"rbf\", gamma=\"auto\", nu=0.05)\n",
    "    ocsvm.fit(X_scaled)\n",
    "    joblib.dump(ocsvm, \"oneclass_svm_model.pkl\")\n",
    "else:\n",
    "    ocsvm = joblib.load(\"oneclass_svm_model.pkl\")\n",
    "df[\"svm_pred\"] = ocsvm.predict(X_scaled)\n",
    "df[\"svm_pred\"] = df[\"svm_pred\"].map({-1: 1, 1: 0})\n",
    "df[\"svm_score\"] = ocsvm.decision_function(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "596dc244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4956/4956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder\n",
    " \n",
    "input_dim = X_scaled.shape[1]\n",
    "encoding_dim = min(6, input_dim // 2)\n",
    "\n",
    "if first_chunk:\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(encoding_dim, activation=\"relu\", activity_regularizer=regularizers.l1(1e-5))(input_layer)\n",
    "    decoder = Dense(input_dim, activation=\"linear\")(encoder)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "    autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    autoencoder.fit(X_scaled, X_scaled, epochs=20, batch_size=64, shuffle=True, verbose=0)\n",
    "    autoencoder.save(\"autoencoder_model.h5\")\n",
    "else:\n",
    "    autoencoder = load_model(\"autoencoder_model.h5\")\n",
    "\n",
    "reconstructions = autoencoder.predict(X_scaled, batch_size=64)\n",
    "mse = np.mean(np.power(X_scaled - reconstructions, 2), axis=1)\n",
    "threshold = np.percentile(mse, 95)\n",
    "df[\"ae_pred\"] = (mse > threshold).astype(int)\n",
    "df[\"ae_score\"] = mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5922f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des modèles\n",
    "\n",
    "results_chunk = {}\n",
    "for model in [\"iso_pred\", \"svm_pred\", \"ae_pred\"]:\n",
    "    anomalies = df[model].sum()\n",
    "    prop = anomalies / len(df)\n",
    "    try:\n",
    "        sil = silhouette_score(X_scaled, df[model])\n",
    "        db = davies_bouldin_score(X_scaled, df[model])\n",
    "    except:\n",
    "        sil, db = None, None\n",
    "    results_chunk[model] = {\n",
    "        \"Anomalies détectées\": anomalies,\n",
    "        \"Proportion\": round(prop, 3),\n",
    "        \"Silhouette\": sil,\n",
    "        \"Davies-Bouldin\": db\n",
    "    }\n",
    "all_results.append(results_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4fd5c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde résultats chunk par chunk\n",
    "\n",
    "df.to_csv(\"RESULTS.csv\", mode='a', index=False, header=first_chunk)\n",
    "first_chunk = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a785e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Résultats comparatifs avec gravité ===\n",
      "                 Anomalies détectées  Proportion (%)  Forte suspicion  \\\n",
      "IsolationForest              15860.0             5.0          15860.0   \n",
      "OneClassSVM                  15861.0             5.0          15860.0   \n",
      "AutoEncoder                  15860.0             5.0          15860.0   \n",
      "\n",
      "                 Faible suspicion  \n",
      "IsolationForest               0.0  \n",
      "OneClassSVM                   1.0  \n",
      "AutoEncoder                   0.0  \n",
      "\n",
      "=== Top 5 anomalies AutoEncoder (fortement suspectes) ===\n",
      "         instanceid     nif_imp      hs_cod  assessed_value_usd      ae_score  \\\n",
      "4124705     2864316  0000012107  84522900.0            72867.25  28151.987167   \n",
      "4303396     2940566  1000002414  38229000.0             1316.52  10940.106134   \n",
      "4125281     2864516  1000018108  44209000.0              169.65   3825.234230   \n",
      "4268181     2925017  3000009906  29349900.0              936.82   1897.212071   \n",
      "4281813     2930368  2000000565  88023000.0         11500000.00   1751.404466   \n",
      "\n",
      "        ae_severity  \n",
      "4124705       Forte  \n",
      "4303396       Forte  \n",
      "4125281       Forte  \n",
      "4268181       Forte  \n",
      "4281813       Forte  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"total_rows = len(df)  # nombre total de lignes dans ton dataset\n",
    "\n",
    "final_results = {}\n",
    "\n",
    "for model in [\"iso_pred\", \"svm_pred\", \"ae_pred\"]:\n",
    "    anomalies_detectees = sum(chunk[model][\"Anomalies détectées\"] for chunk in all_results)\n",
    "    \n",
    "    final_results[model] = {\n",
    "        \"Anomalies détectées\": anomalies_detectees,\n",
    "        \"Proportion\": round(anomalies_detectees / total_rows, 3),\n",
    "        \"Proportion (%)\": round(100 * anomalies_detectees / total_rows, 2)\n",
    "    }\n",
    "\n",
    "# Transformer en DataFrame\n",
    "results_df = pd.DataFrame(final_results).T\n",
    "\n",
    "# Trier par anomalies détectées\n",
    "results_df = results_df.sort_values(by=\"Anomalies détectées\", ascending=False)\n",
    "\n",
    "# Afficher top 5\n",
    "print(\"\\n Comparaison des modèles (Top 5) :\")\n",
    "print(results_df.head(5))\n",
    "\n",
    "# Sauvegarder le tableau en CSV\n",
    "results_df.to_csv(\"comparaison_models.csv\", sep=\";\", encoding=\"utf-8\")\n",
    "\"\"\"\n",
    "\n",
    "# Analyse des anomalies détectées\n",
    "\n",
    "# Seuils automatiques pour chaque modèle\n",
    "iso_threshold = df[\"iso_score\"].quantile(0.05)   # 5% les plus faibles\n",
    "svm_threshold = df[\"svm_score\"].quantile(0.05)   # 5% les plus faibles\n",
    "ae_threshold  = df[\"ae_score\"].quantile(0.95)    # 5% les plus élevées\n",
    "\n",
    "df[\"iso_severity\"] = np.where(df[\"iso_score\"] < iso_threshold, \"Forte\", \"Faible\")\n",
    "df[\"svm_severity\"] = np.where(df[\"svm_score\"] < svm_threshold, \"Forte\", \"Faible\")\n",
    "df[\"ae_severity\"]  = np.where(df[\"ae_score\"]  > ae_threshold, \"Forte\", \"Faible\")\n",
    "\n",
    "\n",
    "# Résumé final avec gravité\n",
    "\n",
    "summary = {}\n",
    "for model, pred_col, sev_col in [\n",
    "    (\"IsolationForest\", \"iso_pred\", \"iso_severity\"),\n",
    "    (\"OneClassSVM\", \"svm_pred\", \"svm_severity\"),\n",
    "    (\"AutoEncoder\", \"ae_pred\", \"ae_severity\")\n",
    "]:\n",
    "    anomalies = df[df[pred_col] == 1]\n",
    "    total = len(df)\n",
    "    summary[model] = {\n",
    "        \"Anomalies détectées\": len(anomalies),\n",
    "        \"Proportion (%)\": round(len(anomalies) / total * 100, 2),\n",
    "        \"Forte suspicion\": (anomalies[sev_col] == \"Forte\").sum(),\n",
    "        \"Faible suspicion\": (anomalies[sev_col] == \"Faible\").sum()\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(summary).T\n",
    "print(\"\\n Résultats comparatifs avec gravité \")\n",
    "print(results_df)\n",
    "\n",
    "# Top 5 anomalies les plus graves (selon AutoEncoder par exemple)\n",
    "\n",
    "top5_ae = df.sort_values(\"ae_score\", ascending=False).head(5)\n",
    "print(\"\\n Top 5 anomalies AutoEncoder (fortement suspectes) \")\n",
    "print(top5_ae[[\"instanceid\", \"nif_imp\", \"hs_cod\", \"assessed_value_usd\", \"ae_score\", \"ae_severity\"]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
